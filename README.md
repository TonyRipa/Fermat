
# Fermat: A Rule System for Constraints

Author:	Anthony John Ripa

Date:	2020.12.15

## Fermat

This project <code>Fermat</code> is an extension of a previous work <code>Leibniz</code>.

<code>Leibniz</code> is an mathematical expression simplifier. Both projects deal with the semantics of x/x. <code>Leibniz</code> uses a kind of generic semantics where x/x=1. This allows algebra to solve problems in calculus like ((x+h)^2-x^2)/h@h=0 is (x^2+2xh+h^2-x^2)/h@h=0 is (2xh+h^2)/h@h=0 is 2x+h@h=0 is 2x. This was only possible because we allowed h/h to be replaced by 1. In other words, the graph h/h does not have a hole in it at h=0.

<code>Leibniz</code> is good for simplifying expressions that do not involve an = sign (i.e. not constraints). How to handle constraints (like x=2x)? Naive extension of <code>Leibniz</code> may allow us to divide both sides by x, to get 1=2. This is sub-optimal. It seems that this semantics demands that x/x have a hole in it at x=0. In this logic we do not have x/x=1. We have x/x = 1 if x≠0 else %. Now, the only allowable transformation for this x=2x would be subtract x from both sides yielding 0=x. The right answer.

So, there are 2 competing semantics. The traditional constraint semantics x/x = 1 if x≠0 else %. And the generic semantics x/x=1. What about problems that intermix these semantics? Like (2x+h) * h = h * m. We are tempted to divide both sides by h, to get 2x+h=m, but constraint semantics disallows this. What to do?

<code>Fermat</code> is a rule-system that correctly deals with both semantics, even when mixed. We simply need to annotate the type, so that we never mix up the semantics. Prolog already already has this concept that capitals like X are unknown, and lowercase like x are atoms. We notate as (2x+h) * h = h * M. Dividing both sides by h is safe. <code>Fermat</code> yields 2x+h=M.

In retrospect, it is simple. For example, i * i = 1 seems to be unintelligible. However, mathematicians may write it as i * <i>i</i> = 1 .  Now it is more clear: <i>i</i> represents the square-root of -1. Whereas, i is something that we can solve for. The solution is i = -<i>i</i> .

We simply have to be clear about the difference (and notate the difference) between the base language (complex, generic, ...) and the meta-language (constraint language with symbols for unknowns).

Alternatively, we may think of it as a hierarchy. Numbers are at the base of this hierarchy. Then a level up we have abstractions over numbers, which we wrote as lowercase x, and we called generics. Then a level up we have abstractions over generics, which we wrote as uppercase X.

## Solution Set

Constraint semantics seems to more explicitly manifest the problem of unexpected solutions than generic semantics does. Consider X=2*X. We may try to divide by X yielding 1=2, and think that there are no solutions. However, our manipulation seems to have lost a solution. Instead we may try to subtract X from both sides yielding 0=X. Now we have 1 solution. Many may be satisfied that we found all the solutions. Let's try for more. Start by reciprocating both sides 1/X=1/(2*X). Pull out the constant factor on the right 1/X=1/2*1/X. Subtract 1/2*1/X from both sides. 1/2*1/X=0. Multiply both sides by 2. 1/X=0. We get X=1/0 as a solution. You may interpret this as satisfying the original constraint by saying that ∞=2∞. Others may say 1/0 is undefined. However, this is circular. √-1 is undefined if you exclude it. The same is true for fractional numbers, negative numbers, and even 0. With generic semantics, simplifying an expression (that does not include an equal sign) we may get √-1 in a less indirect manner (such as simplifying √(0-1) or similar). With expression simplification, it seems that every time that we get an expression that we cannot simplify into one of our other elementary forms, then we now explicitly have a new elementary form on our hands. E.G. 2/3, √2, √-1, 1/0, etc. However, with constraint semantics when we get an answer with a manipulation, there does not appear to be any clear stopping criteria that we should not look for more solutions.

A common approach to dealing with constraints is to fix the solution set before the problem starts. For example, we may only look for solutions that are real numbers (perhaps because we believe that these are the only actualizable solutions, hence the term real). However, we may miss solutions due to our unwarranted initial assumption about the solution space. We may miss complex solutions that may have solved our problem in a meaningful way. Building on this approach, we may keep an open mind of the solution space why doing exploratory analysis of the problem. Then after a time when we are satisfied that we understand the problem and what its solution space should be, then we fix the solution space for this domain. This is an improvement. However, there is a presumption that this process actually limits upon anything.

Let's consider 2 examples in which thinking that we know enough to fix the hypothesis space fails. One is with real numbers. When performing the calculations for the existence of regular polyhedra (the so-called Platonic solid) we assume a real number for the solutions. If so we get 5 Platonic solids. If we use R* the extended reals, we get 6. The sixth is the Sphere. We can't predict beforehand the right solution space. Therefore, we can't predict anything about the shapes which we can expect, or not expect. Making presumptions limits the shapes that we can find. Another example is from Differential Equations. Here we assume complex number solutions. We may want to solve Df=cf. We translate that to (D-c)f=0. We solve corresponding equation D-c=0 or m-c=0. We get m=c and the solution is e^(ct). We may have 2 different roots (D-c₁)(D-c₂)f=0. Here we get solutions e^(c₁t) and e^(c₂t). The roots may be repeated c₁=c₂. In this case instead of e^(c₁t) and e^(c₂t) we get e^(c₁t) and te^(c₂t). This seems to break the format, and should raise a flag, but seems not to have brought into question the solution set. Remember we had the idea that a polynomial of degree n has n roots. However, it is typically allowed to have repeated roots. This is suspicious, and violates the original assumption, with an exception case. Consider x^2=0. Typically it is said to have repeated roots r₁=r₂=0. Let's rethink. x^2=0 seems to have 0 as a solution. A solution should be any number that when squared gives 0. Racking our brain we may recall some work with nilpotent numbers (sometimes written ε) that are not 0 but whose square is 0. Allowing this then we get 2 solutions (as the theorem predicts) to x^2=0. The solutions are r₁=0, and r₁=ε. The solutions are e^(c₁t) and e^(c₂t) which are e^(0t) and e^(εt). The first is 1. The second is e^(εt) = 1+εt+ε^2t/2+ε^3t/6+… = 1+εt+0+0+… = 1+εt. This seems to give us both solutions without special cases. It seems that to solve polynomial constraints, not only should we understand roots of unity, but we should also understand roots of nullity. The moral seems to be that even after centuries (in the Diff.Eq. case) or millennia (in the Platonic case) when we think that we have fully determined the solution space, we can still be wrong.

This being the case it seems that the best we can do is merely return popular solutions, not all of them. In the alternative, instead of returning the solution set, we may return the simplified version of the constraint. Maybe if the input is X^2=1-1 then we return X^2=0 (a simpler constraint) not a solution set like {0} or {0,ε}.

## Constraints vs. Generics

We return to x/x. For generics we think we can safely replace x/x with 1. Constraint semantics suggest that this may be a suboptimal rule because changing X=2X to 1=2 loses the solution X=0. Splintering the semantics suggests x/x=1 but X/X=(x=0)?%:1. Those whose thinking is dominated by constraint semantics may think X/X=(x=0)?%:1 , and not be comfortable with the apparently less cautious alternate domain where x/x=1. However, it seems that the constraint semantics is actually not so cautious. Consider X=2X. Constraint semantics disallows dividing both sides by X. This belief is justified by 2 reasons: 1) is that X/X has a hole in it at X=0 2) We know that X=2X simplified to 1=2 loses a solution (namely X=0). Taken together constraint semantics seems to be cautious. However constraint semantics is typically good with the replacement of X-X with 0. This is not cautious. Consider X=2X. Subtract X from both sides. 0=X. This may look fine. However we lost a solution. X=1/0 satisfies X=2X, but not 0=X. We lost the solution X=1/0. A typical constraint semanticist may think this is fine because 1/0 is not real. However this reasoning is circular. I.E. everything that you choose to exclude is undefined by definition. This does not justify the exclusion. Experience (for centuries or millennia) also cannot justify the exclusion as we saw earlier with Platonic Solids and/or Differential Equations. Furthermore, for solutions of the form a/b (where a & b are integers) if I reciprocate the problem then give it to the solver, then reciprocate the answer, then I should get the right answer. Basically flipping the projective real line upside down should not change the answer. Typical constraint semantics is insufficiently cautious to guarantee this. This is because they allow X-X to be replaced with 0. What should now be clear is that replacing X/X with 1 is as safe as replacing X-X with 0. Either both are safe, or neither. It seems neither.

For generic semantics is seemed that we x/x=1. This may have taken some convincing because constraint semantics suggested there's a hole. However, we never seemed to question x-x=0. This seemed obviously true because constraint semantics had allowed it. However, cautious constraint semantics does not allow it. This suggests that for generics x/x=1 is as safe as x-x=0. For generic semantics we did not ground the semantics in imagined substitutions but on generic quantities. For example x/x is 1 because how many x's per x are there? The answer is 1. Similarly, for x-x=0.

In conclusion, x/x=1 is as true as x-x=0. Also X/X=1 is as true as X-X=0. Objections that emerge from considerations of real numbers are special cases (much like the theorems in real-analysis when compared with complex-analysis). Intuitions about what x/x should be given what X/X should be, when only thinking about reals, are unreliable domain specific intuitions that are half oversafe and half undersafe. Traditional methods have a trade-off profile that has been largely overlooked, likely because it was not fully explored. Fully understanding that trade-off can inform the practitioner as to what approach is more desirable and when. An alternate approach is splitting into 2 semantics: 1) Generic semantics where x/x=1 and x-x=0 can guarantee safety 2) Constraint semantics where neither X/X=1 nor X-X=0 can guarantee safety. If problems can be well-modularized into the different semantics, then it would appear that it may be a good choice to apply the appropriate semantics in the appropriate place, instead of trying a one-size-fits-all, especially when it becomes clear that the one-size-fits-all is not so good a fit.

## Dependencies

<a href='https://github.com/TonyRipa/Leibniz'>Leibniz</a>
